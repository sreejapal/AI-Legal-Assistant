{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fe16b4f-2532-4546-b906-92cc2d27ada8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train=pd.read_csv(\"train.csv\")\n",
    "test=pd.read_csv(\"test.csv\")\n",
    "val=pd.read_csv(\"validation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50d91881-8e37-439a-9834-dcd5c710b791",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chattisgarh_HC_ChattisHC_2019_2061</td>\n",
       "      <td>21/06/2019 Heard.The challenge is to the impug...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kerala_HC_2014_4067</td>\n",
       "      <td>Heard the learned counsel for the petitioner a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Income_Tax_Appellate_2013_1192</td>\n",
       "      <td>PER RAJENDRA, A.M.Challenging the order dt.of ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Orissa_HC_2011_241</td>\n",
       "      <td>not a copy of the award till Secondly, the cop...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rajasthan_HC_Jodhpur_2008_2020_2016_3201</td>\n",
       "      <td>All the above mentioned contempt petitions sha...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   filename  \\\n",
       "0        Chattisgarh_HC_ChattisHC_2019_2061   \n",
       "1                       Kerala_HC_2014_4067   \n",
       "2            Income_Tax_Appellate_2013_1192   \n",
       "3                        Orissa_HC_2011_241   \n",
       "4  Rajasthan_HC_Jodhpur_2008_2020_2016_3201   \n",
       "\n",
       "                                                text  label  \n",
       "0  21/06/2019 Heard.The challenge is to the impug...      1  \n",
       "1  Heard the learned counsel for the petitioner a...      0  \n",
       "2  PER RAJENDRA, A.M.Challenging the order dt.of ...      1  \n",
       "3  not a copy of the award till Secondly, the cop...      0  \n",
       "4  All the above mentioned contempt petitions sha...      0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70d96b4f-e13a-4958-bf0a-39df1212f59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(columns=[\"filename\"],inplace=True)\n",
    "test.drop(columns=[\"filename\"],inplace=True)\n",
    "val.drop(columns=[\"filename\"],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04e3d107-4755-4fc5-afd5-f3fb8f1123de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21/06/2019 Heard.The challenge is to the impug...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Heard the learned counsel for the petitioner a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PER RAJENDRA, A.M.Challenging the order dt.of ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>not a copy of the award till Secondly, the cop...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>All the above mentioned contempt petitions sha...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  21/06/2019 Heard.The challenge is to the impug...      1\n",
       "1  Heard the learned counsel for the petitioner a...      0\n",
       "2  PER RAJENDRA, A.M.Challenging the order dt.of ...      1\n",
       "3  not a copy of the award till Secondly, the cop...      0\n",
       "4  All the above mentioned contempt petitions sha...      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f493b9c-f145-41fd-aef8-84bc7e683a0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The 1st petitioner was an LPG distributor at K...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gowri Shankar, Member (T) M s. Besta Cosmetics...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S. Kang, Heard both sides.filed this appeal ag...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Heard counsel for the parties.The only relief ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>against him by the respondent under Sections 4...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  The 1st petitioner was an LPG distributor at K...      0\n",
       "1  Gowri Shankar, Member (T) M s. Besta Cosmetics...      0\n",
       "2  S. Kang, Heard both sides.filed this appeal ag...      0\n",
       "3  Heard counsel for the parties.The only relief ...      0\n",
       "4  against him by the respondent under Sections 4...      1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2501d250-540a-4a1d-b77b-402fa2771ba9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>charge sheet has been filed in the present cas...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Coram Honble Ramesh C.J.Honble Alok Kumar Verm...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mr. R.P.Nautiyal, Advocate, present for the pe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Shri S.S. Sekhon, Member (T) The Collector had...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M. Kumar, J.This order shall dispose of six ap...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  charge sheet has been filed in the present cas...      1\n",
       "1  Coram Honble Ramesh C.J.Honble Alok Kumar Verm...      0\n",
       "2  Mr. R.P.Nautiyal, Advocate, present for the pe...      0\n",
       "3  Shri S.S. Sekhon, Member (T) The Collector had...      0\n",
       "4  M. Kumar, J.This order shall dispose of six ap...      0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7e35e2f-1e84-4ded-86d1-9493b032f513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\notebooks\\titanic\\venv\\lib\\site-packages (4.57.1)\n",
      "Requirement already satisfied: filelock in c:\\notebooks\\titanic\\venv\\lib\\site-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\notebooks\\titanic\\venv\\lib\\site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\notebooks\\titanic\\venv\\lib\\site-packages (from transformers) (2.3.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\notebooks\\titanic\\venv\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\notebooks\\titanic\\venv\\lib\\site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\notebooks\\titanic\\venv\\lib\\site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in c:\\notebooks\\titanic\\venv\\lib\\site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\notebooks\\titanic\\venv\\lib\\site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\notebooks\\titanic\\venv\\lib\\site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\notebooks\\titanic\\venv\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\notebooks\\titanic\\venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\notebooks\\titanic\\venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: colorama in c:\\notebooks\\titanic\\venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\notebooks\\titanic\\venv\\lib\\site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\notebooks\\titanic\\venv\\lib\\site-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\notebooks\\titanic\\venv\\lib\\site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\notebooks\\titanic\\venv\\lib\\site-packages (from requests->transformers) (2025.10.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90aaeac6-691a-4d4b-9eb7-6444dd811aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
      "Requirement already satisfied: torch in c:\\notebooks\\titanic\\venv\\lib\\site-packages (2.9.1+cu130)\n",
      "Requirement already satisfied: torchvision in c:\\notebooks\\titanic\\venv\\lib\\site-packages (0.24.1+cu130)\n",
      "Requirement already satisfied: filelock in c:\\notebooks\\titanic\\venv\\lib\\site-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\notebooks\\titanic\\venv\\lib\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\notebooks\\titanic\\venv\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\notebooks\\titanic\\venv\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\notebooks\\titanic\\venv\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\notebooks\\titanic\\venv\\lib\\site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: setuptools in c:\\notebooks\\titanic\\venv\\lib\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: numpy in c:\\notebooks\\titanic\\venv\\lib\\site-packages (from torchvision) (2.3.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\notebooks\\titanic\\venv\\lib\\site-packages (from torchvision) (12.0.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\notebooks\\titanic\\venv\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\notebooks\\titanic\\venv\\lib\\site-packages (from jinja2->torch) (3.0.3)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch torchvision --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21d7c20d-f5cb-4e5c-8724-a0712a80cf46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Nov 15 00:37:22 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 581.57                 Driver Version: 581.57         CUDA Version: 13.0     |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3050 ...  WDDM  |   00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   40C    P8              3W /   95W |       0MiB /   6144MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a1b42d5-7b22-4570-870b-c5ee65404bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu130\n",
      "Requirement already satisfied: torch in c:\\notebooks\\titanic\\venv\\lib\\site-packages (2.9.1+cu130)\n",
      "Requirement already satisfied: torchvision in c:\\notebooks\\titanic\\venv\\lib\\site-packages (0.24.1+cu130)\n",
      "Requirement already satisfied: filelock in c:\\notebooks\\titanic\\venv\\lib\\site-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\notebooks\\titanic\\venv\\lib\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\notebooks\\titanic\\venv\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\notebooks\\titanic\\venv\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\notebooks\\titanic\\venv\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\notebooks\\titanic\\venv\\lib\\site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: setuptools in c:\\notebooks\\titanic\\venv\\lib\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: numpy in c:\\notebooks\\titanic\\venv\\lib\\site-packages (from torchvision) (2.3.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\notebooks\\titanic\\venv\\lib\\site-packages (from torchvision) (12.0.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\notebooks\\titanic\\venv\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\notebooks\\titanic\\venv\\lib\\site-packages (from jinja2->torch) (3.0.3)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch torchvision --index-url https://download.pytorch.org/whl/cu130"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76746c6a-5723-4078-a28a-385f566977ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success! GPU is available.\n",
      "Device Name: NVIDIA GeForce RTX 3050 6GB Laptop GPU\n",
      "\n",
      "Current device: 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if a CUDA-enabled GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    # Get the name of the GPU\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    print(f\"✅ Success! GPU is available.\")\n",
    "    print(f\"Device Name: {gpu_name}\")\n",
    "else:\n",
    "    print(\"❌ GPU not found.\")\n",
    "    print(\"PyTorch will use the CPU. This will be much slower for training.\")\n",
    "\n",
    "# You can also print the current device torch is set to use\n",
    "print(f\"\\nCurrent device: {torch.cuda.current_device() if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22c74e64-5f40-46a6-8df7-f8bf9dc0aa35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-> Processing train (rows: 70570)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing batches:   0%|                                                                      | 0/138 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (523 > 512). Running this sequence through the model will result in indexing errors\n",
      "Tokenizing batches: 100%|████████████████████████████████████████████████████████████| 138/138 [00:11<00:00, 11.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train token stats:\n",
      "  min: 55\n",
      "  max: 1468\n",
      "  mean: 353.56766331302254\n",
      "  median: 342.0\n",
      "  90th percentile: 593\n",
      "\n",
      "-> Processing val (rows: 10055)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing batches: 100%|██████████████████████████████████████████████████████████████| 20/20 [00:01<00:00, 14.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val token stats:\n",
      "  min: 58\n",
      "  max: 1319\n",
      "  mean: 351.22575832918943\n",
      "  median: 337.0\n",
      "  90th percentile: 593\n",
      "\n",
      "-> Processing test (rows: 20033)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing batches: 100%|██████████████████████████████████████████████████████████████| 40/40 [00:03<00:00, 12.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test token stats:\n",
      "  min: 56\n",
      "  max: 1590\n",
      "  mean: 355.2926670992862\n",
      "  median: 346.0\n",
      "  90th percentile: 596\n",
      "\n",
      "Top longest in train:\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           text  num_tokens\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                        This appjal by aha hsba-md is direttz-3d dasd 153- 2899, Czmrg.namey the Cams 0 me fV.?,i,.LLE,i, Sf4EBK,.,.VA Eio1akee3 Chif.r21dLga )i5tr.,i11 the impugned judgment, the the petition filed by the Section i31(ia of ha Hiradu Ma.ri21g iEf0r a of divorce of 1.a.,ielty V V 2 .?x2 .h1vieV Eieafd.ieaned Counsei appearing Gr h2par11s a1dV Judgmer, appeiiam i ihat himseif and the f2.aried on at Kalkers ani.g 0 mm 1316 Said wedkbctk, They Maxed -v. happ?2 few 11GnE.hS ani ihereafim.the A him anti 2-1I1eai with him?VA V 1153 meme and his SiSE.She usesd 0 ga .0 her pzxremg gs E 1.M53 a. if .5 ,3 ix haause withsui infarming him azad his meme She used is ve at her whims and fancies.She was 113 coa1g fwd.She was as ihe guesig p10pey.gm strainezi day by day.V appellant Eiied he petition for a saughi for custsdy of ha ch1dri1V.appeare before the trial Ca,11f7cf-SZ.ti, ing her abjection i1i treated her husband fase was that the appeilant of his mother V A his case got himseif examintl Witness as P.W.2 and prodzysed vidvP1.fhe got herself - v as .W.1 fdV did not produce any document.tha pleadings and the an V ec0.4i.,z he41i3 Caurt fermulaied the foilowing points for V fi5S H1.Whatler the is for the iverce?IvT.EN),743 2009 Vxfheizher the pei.31E,imer is fer the cusEj sf his EaiErr1?3, Wfzgxi.arier?8 an of ihe evidzmtit on (.CC)Fd, ihz Cmzrt the fi1Si, tiwg 30ir1E.5 in ihif gi1iV ace,dmgEy, the petiin for diV,) Efa V Learrzed Cu13se1 for th appe1Ej sbn1iidTf ihat the hadfi1ed the offence uner Seaman Q E156 the The the a.ppeIant The of ms According to the learned 1Co.ns(1, of the appellant in the case w0u1f.showVVihA had Inade alga agaZi1fVtVih appeiiemt which amounied to 1Ei3.r iaxrt cansidered.this aspect of the 11.rfj.i- 1 fi.ated as foliowsg had me esp0nrEeri fmci ninaZ ara-353 against the far E2.gtQ7er2,ce ifs 4E8wA IZPZC.The 2.ad prod(6d.he cerigjiea?mpg Q2 he reiaimg ,0 E263 gaid Cmra.3 age 52./1 1 /2 4. ,2 .,x1s.?i34 g29 3x532 E-m.E2er, he E2013 aisa g2?ciz.e(f he 3rgied. .) Qf E1318 order a.e3 amt?me g33.zfi2rz in U, S 25 Cr.P.C Ali She j2Lcz.s., m.sm,.( V helping the pezionesr.disciose .he,czcrz f.i1i3If.VV Qf Em resportc3zTzVfwa,s is position in the Con.roing ?1e whole fzr.rfI .doz.b..fEeV respndem is dz bozsrid.2.d siif sE1o.CEgViI.respect to her ,izai1v ii sfiaulgg z(iT be by using the hs?.,g.fc.rM1rfzc3 in the Qf the P,E Iz.?AVt.f?vc1i and he When ihai.ljeiieg .i?2.e aione shouli f.0ii aid act.I I is ?e 1 h2zZd have the bdfaricse in rmd he cannot Shirk his giving advice to the 1i,V-- Abecause it has Come in the Qf his own wii.n23s that the rs,j3Q rdf1ij from a j7amiZ anci She ?1fj .5303 aibou the agricuf,ura When that.being r.hefac., find as f11.wiiEz hes respondem to grcm,5 i32z3 fo the p(3izm3r.g 2 5./ -1 XX.- ik /5.x 43.3.?.A,Ne.?33 gf 2009 Fhsrs an rscsrd is show ishsi he hszi C)I1SE.JI1d psism is sneti E231 lifs.ii.hm .2-- rsspe1dtriWif-.Was-s Ere22.f.CE s.ii42 gtrufsity by the amd hiss which I1a1ri.is mnsume pzyison, In our the by the appeliam.againsi.the iVI-(.-5 been pmveci.Na is mack?m.--.bj,?iih VV husband .150 grant.        1468\n",
      "              B.CIVIL WRIT PETITION NO.4128/2009 (SMT.KANCHAN RAO ORS.VS. STATE OF RAJ.ORS.)B.CIVIL WRIT PETITION NO.4129/2009 (BHAGWATI LAL ORS.VS. STATE OF RAJ.ORS.)B.CIVIL WRIT PETITION NO.(CHANDA AHIR ORS.VS. STATE OF RAJ.ORS.)B.CIVIL WRIT PETITION NO.(BAJRANG SINGH ORS.VS. STATE OF RAJ.ORS.)B.CIVIL WRIT PETITION NO.(ANAND KUMAR ORS.VS. STATE OF RAJ.ORS.)B.CIVIL WRIT PETITION NO.5327/2009 (BALWANT DAN ANR.VS. STATE OF RAJ.ORS.)B.CIVIL WRIT PETITION NO.5331/2009 (KAILASH KUMAR MUNDEL VS. STATE OF RAJ.ORS.)B.CIVIL WRIT PETITION NO.5339/2009 (SATVEER ANR.VS. STATE OF RAJ.ORS.)B.CIVIL WRIT PETITION NO.5344/2009 (RENUKA ARORA ORS.VS. STATE OF RAJ.ORS.)B.CIVIL WRIT PETITION NO.5347/2009 (SMT.VIMLESH KANWAR VS. STATE OF RAJ.ORS.)B.CIVIL WRIT PETITION NO.(BASANT ORS.VS. STATE OF RAJ.ORS.)B.CIVIL WRIT PETITION NO.5357/2009 (KHETA RAM ORS.VS. STATE OF RAJ.ORS.)B.CIVIL WRIT PETITION NO.5365/2009 (MUKESH RAM ORS.VS. STATE OF RAJ.ORS.)B.CIVIL WRIT PETITION NO.5368/2009 (OMENDER SINGH VS. STATE OF RAJ.ORS.)B.CIVIL WRIT PETITION NO.5372/2009 (SMT.MADHU BALA ORS.VS. STATE OF RAJ.ORS.)B.CIVIL WRIT PETITION NO.5383/2009 (SOHAN DAS SWAMI ANR.VS. STATE OF RAJ.ORS.)B.CIVIL WRIT PETITION NO.5389/2009 RAM VS. STATE OF RAJ.ORS.)B.CIVIL WRIT PETITION NO.(NARPAT RAM ORS.VS. STATE OF RAJ.ORS.)B.CIVIL WRIT PETITION NO.(MEENAKSHI TAK ORS.VS. STATE OF RAJ.ORS.)B.CIVIL WRIT PETITION NO.3741/2009 (ANDA RAM ORS.VS. STATE OF RAJ.ORS.)B.CIVIL WRIT PETITION NO.(HOTU RAM VS. STATE OF RAJ.ORS.)B.CIVIL WRIT PETITION NO.5411/2009 ORS.VS. STATE OF RAJ.ORS.)B.CIVIL WRIT PETITION NO.5417/2009 (IDANA RAM ORS.VS. STATE OF RAJ.ORS.)B.CIVIL WRIT PETITION NO.5418/2009 (KHET DAN CHARAN ANR VS. STATE OF RAJ.ORS.)B.CIVIL WRIT PETITION NO.5424/2009 (KALYAN SINGH ORS.VS. STATE OF RAJ.ORS.)B.CIVIL WRIT PETITION NO.5427/2009 (MAHENDRA KUMAR ORS.VS. STATE OF RAJ.ORS.)B.CIVIL WRIT PETITION NO.5433/2009 (RAM VILASH ORS.VS. STATE OF RAJ.ORS.)B.CIVIL WRIT PETITION NO.5434/2009 (JEEVRAJ VS. STATE OF RAJ.ORS.)B.CIVIL WRIT PETITION NO.5445/2009 (TAN SINGH VS. STATE OF RAJ.ORS.)B.CIVIL WRIT PETITION NO.(RAMESHWAR LAL ORS.VS. STATE OF RAJ.ORS.)B.CIVIL WRIT PETITION NO.5461/2009 (HEM RAJ ORS.VS. STATE OF RAJ.ORS.)B.CIVIL WRIT PETITION NO.5462/2009 (RAM SUKH MIRDH ANR.VS. STATE OF RAJ.ORS.)B.CIVIL WRIT PETITION NO.5466/2009 (ALKA SONI ORS.VS. STATE OF RAJ.ORS.)B.CIVIL WRIT PETITION NO.5472/2009 (RAJENDRA DUKIYA VS. STATE OF RAJ.ORS.)B.CIVIL WRIT PETITION NO.5473/2009 (VIJAY LAXMI ORS.VS. STATE OF RAJ.ORS.)B.CIVIL WRIT PETITION NO.5474/2009 RAM MEENA ORS.VS. STATE OF RAJ.ORS.)B.CIVIL WRIT PETITION NO.5479/2009 (RAKESH BARUPAL VS. STATE OF RAJ.ORS.)B.CIVIL WRIT PETITION NO.(SURENDRA SINGH ORS.VS. STATE OF RAJ.ORS.)B.CIVIL WRIT PETITION NO.(BHANWAR LAL BHATI ORS.VS. STATE OF RAJ.ORS.)Date of Order- May 26 ,2009.PRESENT HONBLE MR. JUSTICE PRAKASH TATIA.Servashri DS Udawat, Rakesh Arora, KS Gill, Subodh Jangid, Shree Kand Verma, DL Motsara, Bhupendra Singh, NA Rajpurohit, Mahendra Inder Jeet, HR Rawal, Deepak Nehra, KS Gill, Gajendra, RS Charan, Rajat Dave, DLR Vyas, Ranjeet Joshi, , TR Choudhary, Manish Purohit, Mrinmay, RS Choudhary, Jogendra Singh, Ravindra Singh, CS Purohit, A Acharya, Rajesh Choudhary, Ramesh Purohit, Sandeep KR Choudhary, Sunil Ranwha, JR Patel, JDS Bhati, Deepak RS Chundawat, DS Udawat, PP Choudhary, for the petitioners.Mr. RL Jangid, Addl.Advocate General with Mr.Rajesh Bhati for the State.BY THE COURT In view of the judgment of this Court in S.B.Civil Writ Petition No.2579/09-Devendra Kumar ors.v. The State ors.        1437\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                             ?71 C.SE-3 bje3aimss b5 the V3rsa E3Ii 15.31?21p3e.21 by Eh i11SL3I zwisizsgg (mi af MFAQQROB 365 200?cfw v?FX On2482OG6 Q19 5-same ja,2dgmnii zzmi 3.ri LE11Ei 2.2OQ pas3si 3 (231.36 NKWCAFC/392O32 0 THE fifie 9f the i,ab.Q1f Qfctczr and Co231miS3i01er far Keiar 31Sri(i Koiar Lc.1Tt-- for short), By the?(ha Commissioner has E1Wij1Idf3d taf Rs.317,685/M to the objiix a1i.rHuith iI1ter(3st thereon 12 and in all Ciaim of f.1iie insurer that the Eegistrauon N0.KA irivcalved in the acCidem.Hence.the to satisfy the awarci passed by whereas it is the Case of the cms,s 4(3iut3t01s f.hat the quantum of compensation the is inacEquat.5 and is vIf3q1i1Ti i( Acczardingiyq the 1aimams and I1f3iSSEt1tCi 0 pII1t the-2 crass V 315 ilhf app-21 respc1ixer. . E /7 MFCR8 352G07 /W f lFx3Ne,4248f2OO6 The brief fae.ef the 21s.-1 ere it is 110. in t.h2-1E (he de(3aed ferzkaiesh21p21 died in .21 reed traffic aeie fhg on It is the e1S v)5.LME1e.f crass tat .ith.e -y Ver1kat,esh21ppa was wcsrking as 21 uIde3.V No.1.- iate Sr by his LRS.He fhevilthy pfief to the accider1t On the elaimams 1033-.t security.tlizay 2 eiaim petition against the, of the offending vehicke claiming ti-9n1persat)n The Come up for Consideration C0m.rniSsioner.The Commissioner, after VapprejiaiLIg the )m-- and on reC0rL the imtome of he at and by 50 of it towards pebsckfili ses has awarcied a eon of Rs.3, aieng with ef EVi?3,FiB 651230?cfw Being xt1 he i11)zgxcI zcigxnerzt 2115 order by the the erase 211the i18Li ilave he cmss appeai 3.7 We have heard the ihe part.ies and perused the careful perusal of the impu,i1.ed erder passed by the 0f Eaxvg much less material i1fegu12irt4y(--a s--.31eh, c0mmit1ed by the C0mnies)neif 1w.tiing the aforesaid COmpafiO3e.ACfD death ef the 4, H0wexe4r3 it case of the insure that the which 23.-is.Ai1VEyVed vi iihe aforesaid wee net.ineured xii1A fact has been si21ted by the axwner efiie vehicle ciuriug his iifeme..Ih.is has b.11 03-sie2ei 210. by the Whe 0 the iI1EEJ1I,.VFA.CRC8 365,j2GD?Cfw MFA.Nc.4248f2D6 iih 2116 Grii as55sed by the C2m1issi0ner 13 iiablc-3 10 be medified.5, As against this iihf LI1SC.1 21ppe21iigL--r the ciaima1tS cross 0bject0r3 in order 0 subi9ant.ia.f,5 H impugr1ed a1c1 0rcEcfVpas.ed -A.E1g Cornmissioner took us throiggh EZ4.P3, charge sheet, EXKPITI the regiLs, by the aluthxjy tmi1e4 bearing registration No.KAw7/T2?I insurance policy of t1s .registration ar2Adubmittd that the the liabaihy on the insurer .ariIp-fsrttsai of the exhibits Q abr3v9.1ifhVf5c0rcis, we are of the considered View the1 after a.ppreciati011 ef the aavidencte an r., r?VC0r itasla recorded a specific finding Gf fact that the b8.IiZ1 NCnKAC??T9797/ fF3??4 was?in fact, invavci in z.h atidni Ehref(r, , st fV3?A.CR3B 3g2g?Cfw MFA, in 021 opir1im, .E m7rdci by I1 is just and prpr 2iIij it ioes 3t eaii Em The C)mmiss3iz1 a.fi,3r ithe recarct has awarded just and maszzenabie Hence, xve do Hat find any good Gut by the appeilargi, izhaurer Qzf by the --rr()sg 3bCt.3 t0 i1t.erf3re with the W1dr passed by the C0mmissij.er.For the foregoing and the appeal c1.        1402\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       This appeal is agains the C(5mm011 jtldggnuent.and 21wz1rd cizgmrd I2w5w()6 in on the lo oi1hc Fri-agj-5.,.1 (ti--1 l(.1g.?Sr.D11.and I Ieruber, I Add.Tribuna, Shimoga 1el-(red to brevity .The and award, awardccd 21 tJ.Rs.,4 at 6 10.21. from thedvatr)f of cieposit.as against the of for Bei11g aggrieved ljy 151 sm1r3..-he prse1atea1 this .appd gun .he gf()u1--ri i,11at.the )mpensati011 21m1rded by loss of Hm-2,1.ds loss of 10 and and Xpt-)I1S(.S 1.1adecuate and V nit reqL,iire1s The brie 1- . of (.2 SE?e1r.Tlie No.1 i1ic.wiib and Appellams 2, 3 and 4 are the of E.11( All the YhIC?E were m.1ior.as on the cf-116 01. claim )(,Ufi()11.They 21 claim of M 131Ii.3.116 15CSpQI1dcnts.that the agriculturist.and was the s0iEai.VVh1re1d family.On the iaz.e1u1 day, 211 E-i3Ac)11th8.OO p.V1ii- whe11t.hC along with his son was i.)wa1ds their house and of river bridg3hih i,11c-2 bus by drivel with high speed.in rash and f.n.ahi1ei cznne from opposite direct.i0n W.a1ic1.(,VC.1(?.As 21 resi.11i.the c.ic(ease(.i file rear wheel of the bus over him.Wi11 hr.wa--si()i1 the way to hospital.ht.to V-th-3 inj uH1tiess.On m of the 1eaih of the d()(.fE1StfC1., the A herein have filed 2. claim seekiilg VA V. of (3 The said Claim had come up before the on 12-5-2006.Ih(3 z11ftc?r the Eear11ed on both si(Ies zaftex the oral and 2-1Vai1ab.E o13.V r(r1d, has aliowed the (1aim )eI.iti011 ()fWtI1(.21) e1a1- in art and awarded compe11sat.iQn ()f1 Rs.,40,QOO KLi1-Jclfsrxj heads with I31 ffbm the date of petition til t1(?Hicig.1th.c VW.h21y.(?hd1i appearing for the Counsel appea1ring for the ftm Eength of time.of the ju(igrne11t and award anda1ier CIiIiC.1 evaluatim1 of the original records, it dec21sc2d aLe Krishnappa was aged abOu V . and was em owning j ancE rfhe T1bLm21 has (.()mmiti.ed a grave in taiimg the im.0me of the at M arlnurn.T119 said incr)me by the is for the tIz1.i E116 (c?(urred in the year im.o i1.t an agai(u1turi.st.we can saeiy tz1ke his income per day or Rs.2.400/ per which m1m,.s i5(2Rsi,2n.800., per Out of the said fg11.c)fm be t,0wa1ds the pers(),ai havirli 110 the J16 of the decisicm.of the Verma and others vs. Delhi and another reported in of the as 50 years and tllIfO1(.the .1 Ac(?1di1gly.the appeilants are e1it1eCiQa.Rss.i.800/e x 12 x we award the se1n1c towards 035 of by the 0 T 73 0.I.he 1xs only Rs.iov-a1c1s loss of and l).ss of love emci affect.im.The a1K.i we avd1I a sum of 1.0 each of 2.3 arld 4 i0v21ja.iQss of love and affeciio1.1.in addit.im1 to awarded by the The Irib1.ma1 has --r1c.d 7ij1 amount towards loss 31 tm-- s1m to be awarded.,1e1-ing A1?3g1fjd (,oi.hev f.21cts and ci1cumst2m(es of the (21s.award 3 sum of a(cordir1g1y.it is awarded- 9 4Asz.zin t IZTi.QQ()O -- towards Xp(3I11SS just .ani and does not call for to the iz21cE and Circumst.a11ces in t.11-.        1329\n",
      "Try out our Premium Member services Virtual Legal Query Alert Service and an ad-free Free for one month and pay only if you like it.State Consumer Disputes Redressal Commission M S.Abida Builders And vs 1.Manisha Himmatrao Satpute And on 1 June, 2015 BEFORE THE HONBLE STATE CONSUMER DISPUTES REDRESSAL COMMISSION, MUMBAI FIRST APPEAL NO.A/14/546 (Arisen out of Order Dated in Consumer Complaint No.106/2013 of District Forum, Kolhapur) M s.Abida Builders and Through Nausad Ibrahim Bobade R o.CS No.1948, E Ward, Hanuman Galli, Kasba Kolhapur.Appellant(s) Versus Manisha Himmatrao Satpute, R o.Chambhar Wada, E Ward, Hanuman Galli, Kasba Bawda, Kolhapur.Shri Gunda Satpute, Since through legal heirs- a) smt.Kamal Satpute, Mr.Pandurang Satpute Mr.Bhanudas Satpute Hindurao Satpute No.2(a) to 2(d) R o.2019, E Ward, Hanuman Galli, Kolhapur.Sonabai Lakshman Satpute R o.2019, E Ward, Hanuman Galli, Kolhapur.Respondent(s) FIRST APPEAL NO.A/14/547 (Arisen out of Order Dated in Consumer Complaint No.107/2013 of District Forum, Kolhapur) M s.Abida Builders and Through Nausad Ibrahim Bobade R o.CS No.1948, E Ward, Hanuman Galli, Kasba Kolhapur Appellant(s) Versus Pandurang Satpute, R o.Chambhar Wada, E Ward, Hanuman Galli, Kasba Bawda, Kolhapur.Shri Gunda Satpute, Since through legal heirs- Smt.Kamal Satpute, Mr.Pandurang Satpute Mr.Bhanudas Satpute Hindurao Satpute No.2(a) to 2(d) R o.2019, E Ward, Hanuman Galli, Kolhapur.Sonabai Lakshman Satpute R o.2019, E Ward, Hanuman Galli, Kolhapur.Respondent(s) FIRST APPEAL NO.A/15/548 (Arisen out of Order Dated in Consumer Complaint No.108/2013 of District Forum, Kolhapur) M s.Abida Builders and Through Nausad Ibrahim Bobade R o.CS No.1948, E Ward, Hanuman Galli, Kasba Kolhapur Appellant(s) Versus Himmatrao Laxman Satpute, R o.Chambhar Wada, E Ward, Hanuman Galli, Kasba Bawda, Kolhapur.Shri Gunda Satpute, Since through legal heirs- a) smt.Kamal Satpute, Mr.Pandurang Satpute Mr.Bhanudas Satpute Hindurao Satpute No.2(a) to 2(d) R o.2019, E Ward, Hanuman Galli, Kolhapur.Sonabai Lakshman Satpute R o.2019, E Ward, Hanuman Galli, Kolhapur.Respondent(s) FIRST APPEAL (Arisen out of Order Dated in Consumer Complaint No.109/2013 of District Forum, Kolhapur) M s.Abida Builders and Through Nausad Ibrahim Bobade R o.CS No.1948, E Ward, Hanuman Galli, Kasba Kolhapur Appellant(s) Versus Hindurao Satpute, R o.Chambhar Wada, E Ward, Hanuman Galli, Kasba Bawda, Kolhapur.Shri Gunda Satpute, Since through legal heirs- a) smt.Kamal Satpute, Mr.Pandurang Satpute Mr.Bhanudas Satpute Hindurao Satpute No.2(a) to 2(d) R o.2019, E Ward, Hanuman Galli, Kolhapur.Sonabai Lakshman Satpute R o.2019, E Ward, Hanuman Galli, Kolhapur.Respondent(s) FIRST APPEAL NO.A/14/450 (Arisen out of Order Dated in Consumer Complaint of District Forum, Kolhapur) M s.Abida Builders and Through Nausad Ibrahim Bobade R o.CS No.1948, E Ward, Hanuman Galli, Kasba Kolhapur Appellant(s) Versus Sonabai Laxman Satpute, R o.Chambhar Wada, E Ward, Hanuman Galli, Kasba Bawda, Kolhapur.Shri Gunda Satpute, Since through legal heirs- a) smt.Kamal Satpute, Mr.Pandurang Satpute Mr.Bhanudas Satpute Hindurao Satpute No.2(a) to 2(d) R o.2019, E Ward, Hanuman Galli, Kolhapur.Sonabai Lakshman Satpute R o.2019, E Ward, Hanuman Galli, Kolhapur.Respondent(s) BEFORE B.Joshi, PRESIDING MEMBER Dhanraj MEMBER For the Appellant None For the Respondents (original complainants) ORDER Per Presiding Judicial Member 1 is present for the respondent no.1/original complainants.Nobody is present for the appellant even on second call.        1278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Install if needed (uncomment if not installed)\n",
    "# !pip install transformers sentencepiece\n",
    "\n",
    "import math\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1) load tokenizer\n",
    "MODEL_NAME = \"law-ai/InLegalBERT\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# 2) helper to batch-tokenize and count tokens\n",
    "def count_tokens_in_df(df, text_col=\"text\", batch_size=512, add_column=True):\n",
    "    \"\"\"\n",
    "    Returns a list of token counts for each row in df[text_col].\n",
    "    If add_column=True, adds a 'num_tokens' column to df (inplace).\n",
    "    \"\"\"\n",
    "    texts = df[text_col].astype(str).tolist()\n",
    "    n = len(texts)\n",
    "    token_counts = [0] * n\n",
    "\n",
    "    # process in batches to avoid huge memory peaks\n",
    "    for i in tqdm(range(0, n, batch_size), desc=\"Tokenizing batches\"):\n",
    "        batch_texts = texts[i : i + batch_size]\n",
    "        # tokenize without padding (we only need lengths); disable truncation so you see full length\n",
    "        enc = tokenizer(batch_texts, add_special_tokens=True, truncation=False, padding=False)\n",
    "        batch_lengths = [len(ids) for ids in enc[\"input_ids\"]]\n",
    "        token_counts[i : i + batch_size] = batch_lengths\n",
    "\n",
    "    if add_column:\n",
    "        df[\"num_tokens\"] = token_counts\n",
    "    return token_counts\n",
    "\n",
    "# 3) Run it on your datasets\n",
    "# (Assumes `train`, `val`, `test` are pandas DataFrames already in the notebook)\n",
    "for name, df in [(\"train\", train), (\"val\", val), (\"test\", test)]:\n",
    "    print(f\"\\n-> Processing {name} (rows: {len(df)})\")\n",
    "    count_tokens_in_df(df, text_col=\"text\", batch_size=512, add_column=True)\n",
    "\n",
    "    # quick stats\n",
    "    print(f\"{name} token stats:\")\n",
    "    print(\"  min:\", int(df[\"num_tokens\"].min()))\n",
    "    print(\"  max:\", int(df[\"num_tokens\"].max()))\n",
    "    print(\"  mean:\", float(df[\"num_tokens\"].mean()))\n",
    "    print(\"  median:\", float(df[\"num_tokens\"].median()))\n",
    "    print(\"  90th percentile:\", int(df[\"num_tokens\"].quantile(0.9)))\n",
    "\n",
    "# 4) Optionally inspect top-longest examples\n",
    "def show_longest(df, k=5):\n",
    "    print(df.sort_values(\"num_tokens\", ascending=False)[[\"text\", \"num_tokens\"]].head(k).to_string(index=False))\n",
    "\n",
    "print(\"\\nTop longest in train:\")\n",
    "show_longest(train, k=5)\n",
    "\n",
    "# 5) Optionally save augmented dataframes\n",
    "# train.to_csv(\"train_with_token_counts.csv\", index=False)\n",
    "# val.to_csv(\"val_with_token_counts.csv\", index=False)\n",
    "# test.to_csv(\"test_with_token_counts.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad413951-5726-45a1-b2c0-f29f0aec5ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 1) Load tokenizer\n",
    "# ------------------------------------------------------\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"law-ai/InLegalBERT\")\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 2) Function to count tokens & filter rows > 512\n",
    "# ------------------------------------------------------\n",
    "def filter_max_tokens(df, column=\"text\", max_len=512, batch_size=512):\n",
    "    texts = df[column].astype(str).tolist()\n",
    "    token_counts = []\n",
    "\n",
    "    # batched tokenization\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Tokenizing\"):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        enc = tokenizer(\n",
    "            batch,\n",
    "            add_special_tokens=True,\n",
    "            truncation=False,\n",
    "            padding=False\n",
    "        )\n",
    "        batch_lens = [len(ids) for ids in enc[\"input_ids\"]]\n",
    "        token_counts.extend(batch_lens)\n",
    "\n",
    "    df[\"num_tokens\"] = token_counts\n",
    "\n",
    "    # filter rows <= max_len\n",
    "    before = len(df)\n",
    "    df = df[df[\"num_tokens\"] <= max_len].reset_index(drop=True)\n",
    "    after = len(df)\n",
    "\n",
    "    print(f\"Filtered: {before - after} rows removed (>{max_len} tokens). Remaining: {after}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 3) Apply to train, val, and test\n",
    "# ------------------------------------------------------\n",
    "print(\"\\nProcessing TRAIN\")\n",
    "train = filter_max_tokens(train, column=\"text\", max_len=512)\n",
    "\n",
    "print(\"\\nProcessing VAL\")\n",
    "val = filter_max_tokens(val, column=\"text\", max_len=512)\n",
    "\n",
    "print(\"\\nProcessing TEST\")\n",
    "test = filter_max_tokens(test, column=\"text\", max_len=512)\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 4) Final counts\n",
    "# ------------------------------------------------------\n",
    "print(\"\\n================ FINAL ROW COUNTS ================\")\n",
    "print(f\"TRAIN rows: {len(train)}\")\n",
    "print(f\"VAL rows:   {len(val)}\")\n",
    "print(f\"TEST rows:  {len(test)}\")\n",
    "print(\"==================================================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ddc9ec0f-7d69-4e4a-ad51-86c2dd319f6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>num_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Heard the learned counsel for the petitioner a...</td>\n",
       "      <td>0</td>\n",
       "      <td>225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>All the above mentioned contempt petitions sha...</td>\n",
       "      <td>0</td>\n",
       "      <td>239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MR. JUSTICE AKHTAR HUSAIN KHAN (ORAL) Present ...</td>\n",
       "      <td>0</td>\n",
       "      <td>415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>THE HONBLE JUSTICE SMT.PRAKASH Anjana Prakash,...</td>\n",
       "      <td>0</td>\n",
       "      <td>361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>To question the correctness of the order dated...</td>\n",
       "      <td>0</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  num_tokens\n",
       "0  Heard the learned counsel for the petitioner a...      0         225\n",
       "1  All the above mentioned contempt petitions sha...      0         239\n",
       "2  MR. JUSTICE AKHTAR HUSAIN KHAN (ORAL) Present ...      0         415\n",
       "3  THE HONBLE JUSTICE SMT.PRAKASH Anjana Prakash,...      0         361\n",
       "4  To question the correctness of the order dated...      0         132"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "923c70f1-86d2-442c-b209-68c98bbd957b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(columns=[\"num_tokens\"],inplace=True)\n",
    "test.drop(columns=[\"num_tokens\"],inplace=True)\n",
    "val.drop(columns=[\"num_tokens\"],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fefef136-ea62-4e77-8e62-e48aacb48f8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Heard the learned counsel for the petitioner a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>All the above mentioned contempt petitions sha...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MR. JUSTICE AKHTAR HUSAIN KHAN (ORAL) Present ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>THE HONBLE JUSTICE SMT.PRAKASH Anjana Prakash,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>To question the correctness of the order dated...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  Heard the learned counsel for the petitioner a...      0\n",
       "1  All the above mentioned contempt petitions sha...      0\n",
       "2  MR. JUSTICE AKHTAR HUSAIN KHAN (ORAL) Present ...      0\n",
       "3  THE HONBLE JUSTICE SMT.PRAKASH Anjana Prakash,...      0\n",
       "4  To question the correctness of the order dated...      0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "09cdec91-ba4e-45aa-8601-5cb8313c3abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== CLASS COUNTS =====\n",
      "\n",
      "TRAIN:\n",
      "label\n",
      "0    28666\n",
      "1    26613\n",
      "Name: count, dtype: int64\n",
      "\n",
      "VAL:\n",
      "label\n",
      "0    4098\n",
      "1    3799\n",
      "Name: count, dtype: int64\n",
      "\n",
      "TEST:\n",
      "label\n",
      "0    8163\n",
      "1    7434\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"===== CLASS COUNTS =====\")\n",
    "\n",
    "print(\"\\nTRAIN:\")\n",
    "print(train[\"label\"].value_counts())\n",
    "\n",
    "print(\"\\nVAL:\")\n",
    "print(val[\"label\"].value_counts())\n",
    "\n",
    "print(\"\\nTEST:\")\n",
    "print(test[\"label\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "22a93581-f417-4db5-833c-326105257227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU name: NVIDIA GeForce RTX 3050 6GB Laptop GPU\n",
      "Loaded InLegalBERT.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU name:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "MODEL_NAME = \"law-ai/InLegalBERT\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"Loaded InLegalBERT.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6953b1e7-9024-4073-b76b-12d45cf47c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample texts:\n",
      "\n",
      "[0] Heard the learned counsel for the petitioner and the learned Public Prosecutor.The petitioner is the accused in Crime No.96 of 2016 of the Maradu Police Station, for the offences under Sections 465, 468 and 471 IPC.The petitioner is the owner of two contract carriages namely and KA-01-AD 7857.The se...\n",
      "\n",
      "[1] All the above mentioned contempt petitions shall stand by this common order as the issue involved is same.Mr. P.R.Singh, learned Additional Advocate General and Mr. Harish Purohit, learned counsel appearing on behalf of the on instructions from Mr. O.P.Bairwa, Law Officer, State Road Transport Jodhp...\n",
      "\n",
      "[2] MR. JUSTICE AKHTAR HUSAIN KHAN (ORAL) Present revision has been filed under Section 17 (1) (b) of the Consumer Protection Act, 1986 against order dated passed by District Consumer Forum in complaint case No.383/10 Mercantile Urban Cooperative Bank Ltd. Vs United India Insurance Co. Ltd. whereby appl...\n"
     ]
    }
   ],
   "source": [
    "# Take first 3 texts from train\n",
    "sample_texts = train[\"text\"].astype(str).iloc[:3].tolist()\n",
    "print(\"Sample texts:\")\n",
    "for i, t in enumerate(sample_texts):\n",
    "    print(f\"\\n[{i}] {t[:300]}...\")  # print first 300 chars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6282f677-9ba5-49d1-885e-6931e1140c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last_hidden_state shape: torch.Size([3, 415, 768])\n",
      "CLS embedding shape: torch.Size([3, 768])\n",
      "CLS embedding (numpy) shape: (3, 768)\n"
     ]
    }
   ],
   "source": [
    "# Tokenize\n",
    "enc = tokenizer(\n",
    "    sample_texts,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    "    return_tensors=\"pt\",\n",
    "    return_attention_mask=True\n",
    ")\n",
    "\n",
    "# Move to device\n",
    "input_ids = enc[\"input_ids\"].to(device)\n",
    "attention_mask = enc[\"attention_mask\"].to(device)\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "# outputs.last_hidden_state shape: (batch_size, seq_len, hidden_size)\n",
    "print(\"last_hidden_state shape:\", outputs.last_hidden_state.shape)\n",
    "\n",
    "# CLS embedding = position 0\n",
    "cls_emb = outputs.last_hidden_state[:, 0, :]   # shape: (batch_size, hidden_size)\n",
    "print(\"CLS embedding shape:\", cls_emb.shape)\n",
    "\n",
    "# Move to CPU and NumPy for later SVM use\n",
    "cls_emb_np = cls_emb.cpu().numpy()\n",
    "print(\"CLS embedding (numpy) shape:\", cls_emb_np.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "346c987b-a219-482c-bf5d-784529c4c350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "GPU name: NVIDIA GeForce RTX 3050 6GB Laptop GPU\n",
      "Found existing tokenizer & model in the environment. Using them.\n",
      "\n",
      "Extracting TRAIN embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting CLS embeddings: 100%|████████████████████████████████████████████████| 1728/1728 [30:58<00:00,  1.08s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_cls shape: (55279, 768)\n",
      "\n",
      "Extracting VAL embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting CLS embeddings: 100%|██████████████████████████████████████████████████| 247/247 [04:26<00:00,  1.08s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_val_cls shape: (7897, 768)\n",
      "\n",
      "Extracting TEST embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting CLS embeddings: 100%|██████████████████████████████████████████████████| 488/488 [08:45<00:00,  1.08s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test_cls shape: (15597, 768)\n",
      "\n",
      "Sanity checks:\n",
      "Train labels counts: {0: 28666, 1: 26613}\n",
      "Shapes (train/val/test): (55279, 768) (7897, 768) (15597, 768)\n",
      "Embedding dtype: float32\n",
      "First train embedding (vector preview, first 6 elements):\n",
      "[-0.10805341 -0.03798809  0.34647015 -0.33456904  0.6474612   0.17861027]\n",
      "\n",
      "Saved embeddings to folder: inlegalbert_cls_embeddings (files: X_train_cls.npy, X_val_cls.npy, X_test_cls.npy)\n"
     ]
    }
   ],
   "source": [
    "# ====== Next step: batch CLS extraction for all splits ======\n",
    "# Requirements: torch, transformers, numpy, tqdm already installed.\n",
    "# Assumes `train`, `val`, `test` DataFrames exist and each has a \"text\" column.\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ----- 0) Device + optional GPU info -----\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "if device.type == \"cuda\":\n",
    "    try:\n",
    "        print(\"GPU name:\", torch.cuda.get_device_name(0))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# ----- 1) Ensure tokenizer + model exist (load if not) -----\n",
    "try:\n",
    "    tokenizer  # noqa: F821\n",
    "    model      # noqa: F821\n",
    "    print(\"Found existing tokenizer & model in the environment. Using them.\")\n",
    "except NameError:\n",
    "    from transformers import AutoTokenizer, AutoModel\n",
    "    MODEL_NAME = \"law-ai/InLegalBERT\"\n",
    "    print(f\"Loading tokenizer & model: {MODEL_NAME} (this may take a minute)...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    print(\"Loaded model & tokenizer.\")\n",
    "\n",
    "# ----- 2) Function to extract CLS embeddings in batches -----\n",
    "def extract_cls_embeddings_from_texts(\n",
    "    texts,\n",
    "    tokenizer,\n",
    "    model,\n",
    "    device,\n",
    "    batch_size=32,\n",
    "    max_length=512,\n",
    "    show_progress=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Given a list of texts, returns numpy array (n_texts, hidden_size) containing CLS embeddings.\n",
    "    Uses outputs.last_hidden_state[:, 0, :] as CLS.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_embs = []\n",
    "    n = len(texts)\n",
    "    iterator = range(0, n, batch_size)\n",
    "    if show_progress:\n",
    "        iterator = tqdm(iterator, desc=\"Extracting CLS embeddings\", unit=\"batch\")\n",
    "\n",
    "    try:\n",
    "        for start in iterator:\n",
    "            batch_texts = texts[start : start + batch_size]\n",
    "            enc = tokenizer(\n",
    "                batch_texts,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=max_length,\n",
    "                return_tensors=\"pt\",\n",
    "                return_attention_mask=True\n",
    "            )\n",
    "            input_ids = enc[\"input_ids\"].to(device)\n",
    "            attention_mask = enc[\"attention_mask\"].to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                cls = outputs.last_hidden_state[:, 0, :].cpu().numpy()  # (b, hidden)\n",
    "                all_embs.append(cls)\n",
    "\n",
    "        all_embs = np.vstack(all_embs)\n",
    "        return all_embs\n",
    "\n",
    "    except RuntimeError as e:\n",
    "        # Likely OOM\n",
    "        err = str(e).lower()\n",
    "        if \"out of memory\" in err or \"cuda\" in err:\n",
    "            print(\"\\nRuntimeError (likely OOM). Try reducing batch_size or use CPU.\")\n",
    "            print(\"Suggested actions:\")\n",
    "            print(\" - set batch_size to a smaller value (e.g., 8 or 4).\")\n",
    "            print(\" - or run on CPU: device = torch.device('cpu') and move model to CPU.\")\n",
    "        raise\n",
    "\n",
    "# ----- 3) Run extraction for train/val/test -----\n",
    "# Choose a safe default batch size. If you have a big GPU, increase it.\n",
    "DEFAULT_BATCH_SIZE = 32\n",
    "\n",
    "print(\"\\nExtracting TRAIN embeddings...\")\n",
    "train_texts = train[\"text\"].astype(str).tolist()\n",
    "X_train_cls = extract_cls_embeddings_from_texts(\n",
    "    train_texts, tokenizer, model, device, batch_size=DEFAULT_BATCH_SIZE, max_length=512\n",
    ")\n",
    "print(\"X_train_cls shape:\", X_train_cls.shape)\n",
    "\n",
    "print(\"\\nExtracting VAL embeddings...\")\n",
    "val_texts = val[\"text\"].astype(str).tolist()\n",
    "X_val_cls = extract_cls_embeddings_from_texts(\n",
    "    val_texts, tokenizer, model, device, batch_size=DEFAULT_BATCH_SIZE, max_length=512\n",
    ")\n",
    "print(\"X_val_cls shape:\", X_val_cls.shape)\n",
    "\n",
    "print(\"\\nExtracting TEST embeddings...\")\n",
    "test_texts = test[\"text\"].astype(str).tolist()\n",
    "X_test_cls = extract_cls_embeddings_from_texts(\n",
    "    test_texts, tokenizer, model, device, batch_size=DEFAULT_BATCH_SIZE, max_length=512\n",
    ")\n",
    "print(\"X_test_cls shape:\", X_test_cls.shape)\n",
    "\n",
    "# ----- 4) Quick sanity previews -----\n",
    "print(\"\\nSanity checks:\")\n",
    "print(\"Train labels counts:\", train['label'].value_counts().to_dict())\n",
    "print(\"Shapes (train/val/test):\", X_train_cls.shape, X_val_cls.shape, X_test_cls.shape)\n",
    "print(\"Embedding dtype:\", X_train_cls.dtype)\n",
    "print(\"First train embedding (vector preview, first 6 elements):\")\n",
    "print(X_train_cls[0, :6])\n",
    "\n",
    "# ----- 5) Optionally save embeddings to disk (uncomment to enable) -----\n",
    "out_dir = \"inlegalbert_cls_embeddings\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "np.save(os.path.join(out_dir, \"X_train_cls.npy\"), X_train_cls)\n",
    "np.save(os.path.join(out_dir, \"X_val_cls.npy\"), X_val_cls)\n",
    "np.save(os.path.join(out_dir, \"X_test_cls.npy\"), X_test_cls)\n",
    "print(f\"\\nSaved embeddings to folder: {out_dir} (files: X_train_cls.npy, X_val_cls.npy, X_test_cls.npy)\")\n",
    "\n",
    "# Done — next step will be scaling + training SVM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "26f98f40-a248-450d-b282-5f1880dc9f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings found in memory.\n",
      "Shapes (train/val/test): (55279, 768) (7897, 768) (15597, 768)\n",
      "Label counts (train): [28666 26613]\n",
      "Label counts (val): [4098 3799]\n",
      "Label counts (test): [8163 7434]\n",
      "\n",
      "Starting GridSearchCV for LinearSVC (this may take a few minutes)...\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "\n",
      "Best params: {'C': 0.01}\n",
      "\n",
      "--- VAL results ---\n",
      "Accuracy: 0.7021653792579461\n",
      "Macro F1: 0.7011201631294279\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.73      0.72      4098\n",
      "           1       0.70      0.67      0.68      3799\n",
      "\n",
      "    accuracy                           0.70      7897\n",
      "   macro avg       0.70      0.70      0.70      7897\n",
      "weighted avg       0.70      0.70      0.70      7897\n",
      "\n",
      "Confusion matrix:\n",
      "[[3006 1092]\n",
      " [1260 2539]]\n",
      "\n",
      "--- TEST results ---\n",
      "Accuracy: 0.6974418157337949\n",
      "Macro F1: 0.6958566562880777\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.74      0.72      8163\n",
      "           1       0.69      0.66      0.67      7434\n",
      "\n",
      "    accuracy                           0.70     15597\n",
      "   macro avg       0.70      0.70      0.70     15597\n",
      "weighted avg       0.70      0.70      0.70     15597\n",
      "\n",
      "Confusion matrix:\n",
      "[[6002 2161]\n",
      " [2558 4876]]\n",
      "\n",
      "Saved model and scaler to inlegalbert_svm_model\n",
      "\n",
      "SUMMARY:\n",
      "Best C: 0.01\n",
      "VAL metrics: {'accuracy': 0.7021653792579461, 'f1_macro': 0.7011201631294279}\n",
      "TEST metrics: {'accuracy': 0.6974418157337949, 'f1_macro': 0.6958566562880777}\n"
     ]
    }
   ],
   "source": [
    "# === Train Linear SVM on CLS embeddings ===\n",
    "# Requirements: scikit-learn, joblib, numpy (should already be installed)\n",
    "# This cell assumes X_train_cls, X_val_cls, X_test_cls, and train/val/test DataFrames exist in the session.\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "import joblib\n",
    "\n",
    "# --------- 0) Load embeddings if variables aren't in memory ----------\n",
    "try:\n",
    "    X_train_cls  # noqa: F821\n",
    "    X_val_cls\n",
    "    X_test_cls\n",
    "    print(\"Embeddings found in memory.\")\n",
    "except NameError:\n",
    "    print(\"Embeddings not found in memory — loading from inlegalbert_cls_embeddings/*.npy\")\n",
    "    X_train_cls = np.load(\"inlegalbert_cls_embeddings/X_train_cls.npy\")\n",
    "    X_val_cls = np.load(\"inlegalbert_cls_embeddings/X_val_cls.npy\")\n",
    "    X_test_cls = np.load(\"inlegalbert_cls_embeddings/X_test_cls.npy\")\n",
    "\n",
    "# Labels\n",
    "y_train = train[\"label\"].to_numpy()\n",
    "y_val = val[\"label\"].to_numpy()\n",
    "y_test = test[\"label\"].to_numpy()\n",
    "\n",
    "print(\"Shapes (train/val/test):\", X_train_cls.shape, X_val_cls.shape, X_test_cls.shape)\n",
    "print(\"Label counts (train):\", np.bincount(y_train))\n",
    "print(\"Label counts (val):\", np.bincount(y_val))\n",
    "print(\"Label counts (test):\", np.bincount(y_test))\n",
    "\n",
    "# --------- 1) Scale features ----------\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_cls)\n",
    "X_val_scaled = scaler.transform(X_val_cls)\n",
    "X_test_scaled = scaler.transform(X_test_cls)\n",
    "\n",
    "# --------- 2) Grid search for LinearSVC ----------\n",
    "param_grid = {\"C\": [0.01, 0.1, 1.0, 10.0]}\n",
    "base_svc = LinearSVC(max_iter=10000, tol=1e-4)   # increase max_iter if you get convergence warnings\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator=base_svc,\n",
    "    param_grid=param_grid,\n",
    "    cv=3,\n",
    "    scoring=\"f1_macro\",\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "print(\"\\nStarting GridSearchCV for LinearSVC (this may take a few minutes)...\")\n",
    "grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"\\nBest params:\", grid.best_params_)\n",
    "best_svc = grid.best_estimator_\n",
    "\n",
    "# --------- 3) Evaluate helper ----------\n",
    "def evaluate_model(model, X, y, split_name=\"VAL\"):\n",
    "    y_pred = model.predict(X)\n",
    "    acc = accuracy_score(y, y_pred)\n",
    "    f1_macro = f1_score(y, y_pred, average=\"macro\")\n",
    "    print(f\"\\n--- {split_name} results ---\")\n",
    "    print(\"Accuracy:\", acc)\n",
    "    print(\"Macro F1:\", f1_macro)\n",
    "    print(\"Classification report:\")\n",
    "    print(classification_report(y, y_pred))\n",
    "    print(\"Confusion matrix:\")\n",
    "    print(confusion_matrix(y, y_pred))\n",
    "    return {\"accuracy\": acc, \"f1_macro\": f1_macro}\n",
    "\n",
    "# Evaluate on validation set\n",
    "val_metrics = evaluate_model(best_svc, X_val_scaled, y_val, split_name=\"VAL\")\n",
    "\n",
    "# Evaluate on test set\n",
    "test_metrics = evaluate_model(best_svc, X_test_scaled, y_test, split_name=\"TEST\")\n",
    "\n",
    "# --------- 4) Save artifacts ----------\n",
    "out_dir = \"inlegalbert_svm_model\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "joblib.dump(best_svc, os.path.join(out_dir, \"linear_svc_best.joblib\"))\n",
    "joblib.dump(scaler, os.path.join(out_dir, \"scaler.joblib\"))\n",
    "print(f\"\\nSaved model and scaler to {out_dir}\")\n",
    "\n",
    "# --------- 5) Optional: show top-level summary ----------\n",
    "print(\"\\nSUMMARY:\")\n",
    "print(\"Best C:\", grid.best_params_[\"C\"])\n",
    "print(\"VAL metrics:\", val_metrics)\n",
    "print(\"TEST metrics:\", test_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "087e9fd6-fc26-4c38-aa80-e2c2b846ff7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings found in memory.\n",
      "Shapes: (55279, 768) (7897, 768) (15597, 768)\n",
      "Existing saved SVM test macro-F1: 0.6959\n",
      "xgboost not available; skipping XGBoost. Install with `pip install xgboost` to enable.\n",
      "\n",
      ">> Tuning Logistic Regression (on scaled embeddings)...\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "\n",
      "LogisticRegression best params: {'C': 0.01, 'penalty': 'l2'} (fit time: 1059.1s)\n",
      "LogisticRegression VAL macro-F1: 0.7024 | TEST macro-F1: 0.6968\n",
      "Val classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.74      0.72      4098\n",
      "           1       0.70      0.67      0.68      3799\n",
      "\n",
      "    accuracy                           0.70      7897\n",
      "   macro avg       0.70      0.70      0.70      7897\n",
      "weighted avg       0.70      0.70      0.70      7897\n",
      "\n",
      "Test classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.74      0.72      8163\n",
      "           1       0.69      0.66      0.67      7434\n",
      "\n",
      "    accuracy                           0.70     15597\n",
      "   macro avg       0.70      0.70      0.70     15597\n",
      "weighted avg       0.70      0.70      0.70     15597\n",
      "\n",
      "\n",
      ">> Tuning Random Forest (on raw embeddings)...\n",
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "\n",
      "RandomForest best params: {'max_depth': 50, 'min_samples_split': 5, 'n_estimators': 500} (fit time: 1583.8s)\n",
      "RandomForest VAL macro-F1: 0.7367 | TEST macro-F1: 0.7374\n",
      "Val classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.84      0.77      4098\n",
      "           1       0.78      0.64      0.70      3799\n",
      "\n",
      "    accuracy                           0.74      7897\n",
      "   macro avg       0.75      0.74      0.74      7897\n",
      "weighted avg       0.75      0.74      0.74      7897\n",
      "\n",
      "Test classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.84      0.77      8163\n",
      "           1       0.78      0.64      0.70      7434\n",
      "\n",
      "    accuracy                           0.74     15597\n",
      "   macro avg       0.75      0.74      0.74     15597\n",
      "weighted avg       0.75      0.74      0.74     15597\n",
      "\n",
      "\n",
      "=== SUMMARY of all models (test macro-F1) ===\n",
      "LogisticRegression: test_f1 = 0.6968, val_f1 = 0.7024, fit_time_s = 1059.1\n",
      "RandomForest: test_f1 = 0.7374, val_f1 = 0.7367, fit_time_s = 1583.8\n",
      "Existing saved SVM test macro-F1: 0.6959\n",
      "\n",
      "Best model: RandomForest with TEST macro-F1 = 0.7374\n",
      "Saved best model to: models_compare/best_model_RandomForest.joblib\n",
      "New model outperforms saved SVM (0.6959). Overwriting inlegalbert_svm_model/linear_svc_best.joblib with new model.\n",
      "Overwrite complete.\n",
      "\n",
      "Training VotingClassifier (hard) on train set...\n",
      "Ensemble TEST macro-F1: 0.7061303549071618\n",
      "Saved ensemble as models_compare/voting_ensemble_hard.joblib\n",
      "\n",
      "Done. Check models_compare/ and inlegalbert_svm_model/ for saved artifacts.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import joblib\n",
    "from time import time\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ------------------ 0) load embeddings & labels if needed ------------------\n",
    "try:\n",
    "    X_train_cls  # noqa\n",
    "    X_val_cls\n",
    "    X_test_cls\n",
    "    print(\"Embeddings found in memory.\")\n",
    "except NameError:\n",
    "    print(\"Loading embeddings from disk...\")\n",
    "    X_train_cls = np.load(\"inlegalbert_cls_embeddings/X_train_cls.npy\")\n",
    "    X_val_cls = np.load(\"inlegalbert_cls_embeddings/X_val_cls.npy\")\n",
    "    X_test_cls = np.load(\"inlegalbert_cls_embeddings/X_test_cls.npy\")\n",
    "\n",
    "y_train = train[\"label\"].to_numpy()\n",
    "y_val = val[\"label\"].to_numpy()\n",
    "y_test = test[\"label\"].to_numpy()\n",
    "\n",
    "print(\"Shapes:\", X_train_cls.shape, X_val_cls.shape, X_test_cls.shape)\n",
    "\n",
    "# ------------------ 1) Prepare scaled versions (for logistic) ------------------\n",
    "scaler_log = StandardScaler()\n",
    "X_train_log = scaler_log.fit_transform(X_train_cls)\n",
    "X_val_log = scaler_log.transform(X_val_cls)\n",
    "X_test_log = scaler_log.transform(X_test_cls)\n",
    "\n",
    "# Save scaler if needed\n",
    "os.makedirs(\"models_compare\", exist_ok=True)\n",
    "joblib.dump(scaler_log, \"models_compare/scaler_log.joblib\")\n",
    "\n",
    "# ------------------ 2) Load existing SVM test f1 (if available) ------------------\n",
    "best_existing_f1 = None\n",
    "svm_path = \"inlegalbert_svm_model/linear_svc_best.joblib\"\n",
    "if os.path.exists(svm_path):\n",
    "    try:\n",
    "        # We don't have stored metrics file; if you saved them, load; else we can re-evaluate SVM\n",
    "        existing_svc = joblib.load(svm_path)\n",
    "        # Scale test data using saved scaler if exists\n",
    "        scaler_path = \"inlegalbert_svm_model/scaler.joblib\"\n",
    "        if os.path.exists(scaler_path):\n",
    "            existing_scaler = joblib.load(scaler_path)\n",
    "            X_test_for_svm = existing_scaler.transform(X_test_cls)\n",
    "        else:\n",
    "            X_test_for_svm = X_test_log  # fallback\n",
    "        y_pred_svm = existing_svc.predict(X_test_for_svm)\n",
    "        best_existing_f1 = f1_score(y_test, y_pred_svm, average=\"macro\")\n",
    "        print(f\"Existing saved SVM test macro-F1: {best_existing_f1:.4f}\")\n",
    "    except Exception:\n",
    "        best_existing_f1 = None\n",
    "\n",
    "# ------------------ 3) Define models and search grids ------------------\n",
    "results = {}\n",
    "\n",
    "# 3.a Logistic Regression (GridSearch)\n",
    "lr = LogisticRegression(max_iter=2000, solver=\"saga\", random_state=42)\n",
    "lr_param_grid = {\n",
    "    \"C\": [0.01, 0.1, 1.0, 10.0],\n",
    "    \"penalty\": [\"l2\"],  # saga supports l1/l2/elasticnet - l2 is robust; add l1 if you want sparsity\n",
    "}\n",
    "\n",
    "# 3.b Random Forest (GridSearch)\n",
    "rf = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "rf_param_grid = {\n",
    "    \"n_estimators\": [200, 500],\n",
    "    \"max_depth\": [None, 20, 50],\n",
    "    \"min_samples_split\": [2, 5],\n",
    "}\n",
    "\n",
    "# 3.c XGBoost (RandomizedSearch for speed)\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    xgb_available = True\n",
    "except Exception:\n",
    "    xgb_available = False\n",
    "    print(\"xgboost not available; skipping XGBoost. Install with `pip install xgboost` to enable.\")\n",
    "\n",
    "if xgb_available:\n",
    "    xgb = XGBClassifier(objective=\"binary:logistic\", use_label_encoder=False, eval_metric=\"logloss\", tree_method=\"auto\", random_state=42)\n",
    "    xgb_param_dist = {\n",
    "        \"n_estimators\": [100, 200, 400],\n",
    "        \"max_depth\": [3, 6, 10],\n",
    "        \"learning_rate\": [0.01, 0.05, 0.1],\n",
    "        \"subsample\": [0.6, 0.8, 1.0],\n",
    "        \"colsample_bytree\": [0.6, 0.8, 1.0],\n",
    "        \"reg_alpha\": [0, 0.1, 1],\n",
    "        \"reg_lambda\": [1, 5, 10],\n",
    "    }\n",
    "\n",
    "# Utility to fit, evaluate and store best model\n",
    "def fit_and_eval(clf, Xtr, ytr, Xval, yval, Xte, yte, searcher, model_name):\n",
    "    t0 = time()\n",
    "    searcher.fit(Xtr, ytr)\n",
    "    elapsed = time() - t0\n",
    "    best = searcher.best_estimator_\n",
    "    print(f\"\\n{model_name} best params: {searcher.best_params_} (fit time: {elapsed:.1f}s)\")\n",
    "    # Evaluate on val & test\n",
    "    yval_pred = best.predict(Xval)\n",
    "    ytest_pred = best.predict(Xte)\n",
    "    val_f1 = f1_score(yval, yval_pred, average=\"macro\")\n",
    "    test_f1 = f1_score(yte, ytest_pred, average=\"macro\")\n",
    "    print(f\"{model_name} VAL macro-F1: {val_f1:.4f} | TEST macro-F1: {test_f1:.4f}\")\n",
    "    print(\"Val classification report:\")\n",
    "    print(classification_report(yval, yval_pred))\n",
    "    print(\"Test classification report:\")\n",
    "    print(classification_report(yte, ytest_pred))\n",
    "    # store\n",
    "    results[model_name] = {\n",
    "        \"best_estimator\": best,\n",
    "        \"best_params\": searcher.best_params_,\n",
    "        \"val_f1\": val_f1,\n",
    "        \"test_f1\": test_f1,\n",
    "        \"fit_time_s\": elapsed\n",
    "    }\n",
    "    return best\n",
    "\n",
    "# ------------------ 4) Run GridSearch / RandomizedSearch ------------------\n",
    "# NOTE: adjust cv and n_iter for speed if needed\n",
    "cv_folds = 3\n",
    "\n",
    "# Logistic Regression\n",
    "print(\"\\n>> Tuning Logistic Regression (on scaled embeddings)...\")\n",
    "lr_search = GridSearchCV(lr, lr_param_grid, scoring=\"f1_macro\", cv=cv_folds, n_jobs=-1, verbose=2)\n",
    "best_lr = fit_and_eval(lr, X_train_log, y_train, X_val_log, y_val, X_test_log, y_test, lr_search, \"LogisticRegression\")\n",
    "\n",
    "# Random Forest\n",
    "print(\"\\n>> Tuning Random Forest (on raw embeddings)...\")\n",
    "rf_search = GridSearchCV(rf, rf_param_grid, scoring=\"f1_macro\", cv=cv_folds, n_jobs=-1, verbose=2)\n",
    "best_rf = fit_and_eval(rf, X_train_cls, y_train, X_val_cls, y_val, X_test_cls, y_test, rf_search, \"RandomForest\")\n",
    "\n",
    "# XGBoost (if available) - use RandomizedSearchCV for speed\n",
    "best_xgb = None\n",
    "if xgb_available:\n",
    "    from sklearn.model_selection import RandomizedSearchCV\n",
    "    print(\"\\n>> Tuning XGBoost (on raw embeddings) with RandomizedSearchCV...\")\n",
    "    xgb_search = RandomizedSearchCV(xgb, xgb_param_dist, n_iter=20, scoring=\"f1_macro\", cv=cv_folds, n_jobs=-1, verbose=2, random_state=42)\n",
    "    best_xgb = fit_and_eval(xgb, X_train_cls, y_train, X_val_cls, y_val, X_test_cls, y_test, xgb_search, \"XGBoost\")\n",
    "\n",
    "# ------------------ 5) Compare to existing SVM ------------------\n",
    "# Determine best model by TEST macro-F1\n",
    "print(\"\\n=== SUMMARY of all models (test macro-F1) ===\")\n",
    "for name, info in results.items():\n",
    "    print(f\"{name}: test_f1 = {info['test_f1']:.4f}, val_f1 = {info['val_f1']:.4f}, fit_time_s = {info['fit_time_s']:.1f}\")\n",
    "\n",
    "# Include existing SVM metric if we computed it earlier\n",
    "if best_existing_f1 is not None:\n",
    "    print(f\"Existing saved SVM test macro-F1: {best_existing_f1:.4f}\")\n",
    "\n",
    "# Find best new model\n",
    "best_model_name = None\n",
    "best_model_info = None\n",
    "best_test_f1 = best_existing_f1 if best_existing_f1 is not None else -1.0\n",
    "\n",
    "for name, info in results.items():\n",
    "    if info[\"test_f1\"] > best_test_f1:\n",
    "        best_test_f1 = info[\"test_f1\"]\n",
    "        best_model_name = name\n",
    "        best_model_info = info\n",
    "\n",
    "if best_model_name is None:\n",
    "    print(\"\\nNo new model improved over existing SVM (or no SVM baseline found).\")\n",
    "else:\n",
    "    print(f\"\\nBest model: {best_model_name} with TEST macro-F1 = {best_test_f1:.4f}\")\n",
    "    # Save the best model (overwrite SVM or write new file)\n",
    "    save_name = f\"models_compare/best_model_{best_model_name}.joblib\"\n",
    "    joblib.dump(best_model_info[\"best_estimator\"], save_name)\n",
    "    print(f\"Saved best model to: {save_name}\")\n",
    "\n",
    "    # Option: if it beats existing SVM, offer to overwrite SVM file for consistency\n",
    "    if best_existing_f1 is not None and best_test_f1 > best_existing_f1:\n",
    "        print(f\"New model outperforms saved SVM ({best_existing_f1:.4f}). Overwriting {svm_path} with new model.\")\n",
    "        joblib.dump(best_model_info[\"best_estimator\"], svm_path)\n",
    "        # Save scaler if model requires scaling (Logistic)\n",
    "        if best_model_name == \"LogisticRegression\":\n",
    "            joblib.dump(scaler_log, \"inlegalbert_svm_model/scaler.joblib\")  # reuse path so future loading works\n",
    "        print(\"Overwrite complete.\")\n",
    "\n",
    "# ------------------ 6) Optional: build an ensemble (hard voting) if multiple models are strong ------------------\n",
    "# Build hard-voting ensemble of top 3 models if they all improve over SVM or are competitive\n",
    "# Voting requires estimators with predict(); fine for these models.\n",
    "top_models = sorted(results.items(), key=lambda kv: kv[1][\"test_f1\"], reverse=True)[:3]\n",
    "if len(top_models) >= 2:\n",
    "    estimators_for_ensemble = [(name, info[\"best_estimator\"]) for name, info in top_models]\n",
    "    voting = VotingClassifier(estimators=estimators_for_ensemble, voting=\"hard\", n_jobs=-1)\n",
    "    print(\"\\nTraining VotingClassifier (hard) on train set...\")\n",
    "    voting.fit(X_train_cls, y_train)  # trees/logistic can accept raw embeddings; logistic was fit on scaled, but VotingClassifier expects same input for all. Use raw embeddings here.\n",
    "    ytest_pred = voting.predict(X_test_cls)\n",
    "    ensemble_f1 = f1_score(y_test, ytest_pred, average=\"macro\")\n",
    "    print(\"Ensemble TEST macro-F1:\", ensemble_f1)\n",
    "    joblib.dump(voting, \"models_compare/voting_ensemble_hard.joblib\")\n",
    "    print(\"Saved ensemble as models_compare/voting_ensemble_hard.joblib\")\n",
    "\n",
    "print(\"\\nDone. Check models_compare/ and inlegalbert_svm_model/ for saved artifacts.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ebd08dff-6968-4bfe-a4fb-c1782aceaf8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-3.1.1-py3-none-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy in c:\\notebooks\\titanic\\venv\\lib\\site-packages (from xgboost) (2.3.4)\n",
      "Requirement already satisfied: scipy in c:\\notebooks\\titanic\\venv\\lib\\site-packages (from xgboost) (1.16.2)\n",
      "Downloading xgboost-3.1.1-py3-none-win_amd64.whl (72.0 MB)\n",
      "   ---------------------------------------- 0.0/72.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/72.0 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 5.5/72.0 MB 25.6 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 12.6/72.0 MB 31.1 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 31.7/72.0 MB 48.5 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 48.0/72.0 MB 55.3 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 61.3/72.0 MB 57.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  71.8/72.0 MB 58.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 72.0/72.0 MB 53.7 MB/s  0:00:01\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-3.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8aa5245-a58c-4a12-bded-10164ec26a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings found in memory.\n",
      "Shapes: (55279, 768) (7897, 768) (15597, 768)\n",
      "Label counts (train): [36302 34268]\n",
      "Could not evaluate existing saved model: Found input variables with inconsistent numbers of samples: [20033, 15597]\n",
      "\n",
      ">> Running search for LogisticRegression ...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [55279, 70570]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 131\u001b[39m\n\u001b[32m    128\u001b[39m results = []\n\u001b[32m    130\u001b[39m \u001b[38;5;66;03m# Logistic (on scaled embeddings)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m res_log = \u001b[43mrun_grid_search\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mLogisticRegression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpipe_log\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_grid_log\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_cls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val_cls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test_cls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_random\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    132\u001b[39m results.append(res_log)\n\u001b[32m    134\u001b[39m \u001b[38;5;66;03m# Linear SVC (on scaled embeddings)\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 112\u001b[39m, in \u001b[36mrun_grid_search\u001b[39m\u001b[34m(name, estimator, param_grid, Xtr, ytr, Xval, yval, Xte, yte, use_random, n_iter)\u001b[39m\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    111\u001b[39m     search = GridSearchCV(estimator, param_grid, scoring=\u001b[33m\"\u001b[39m\u001b[33mf1_macro\u001b[39m\u001b[33m\"\u001b[39m, cv=\u001b[32m3\u001b[39m, n_jobs=-\u001b[32m1\u001b[39m, verbose=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m112\u001b[39m \u001b[43msearch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXtr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mytr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    113\u001b[39m elapsed = time() - t0\n\u001b[32m    114\u001b[39m best = search.best_estimator_\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\notebooks\\titanic\\venv\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\notebooks\\titanic\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:955\u001b[39m, in \u001b[36mBaseSearchCV.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m    952\u001b[39m estimator = \u001b[38;5;28mself\u001b[39m.estimator\n\u001b[32m    953\u001b[39m scorers, refit_metric = \u001b[38;5;28mself\u001b[39m._get_scorers()\n\u001b[32m--> \u001b[39m\u001b[32m955\u001b[39m X, y = \u001b[43mindexable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    956\u001b[39m params = _check_method_params(X, params=params)\n\u001b[32m    958\u001b[39m routed_params = \u001b[38;5;28mself\u001b[39m._get_routed_params_for_fit(params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\notebooks\\titanic\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:530\u001b[39m, in \u001b[36mindexable\u001b[39m\u001b[34m(*iterables)\u001b[39m\n\u001b[32m    500\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Make arrays indexable for cross-validation.\u001b[39;00m\n\u001b[32m    501\u001b[39m \n\u001b[32m    502\u001b[39m \u001b[33;03mChecks consistent length, passes through None, and ensures that everything\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    526\u001b[39m \u001b[33;03m[[1, 2, 3], array([2, 3, 4]), None, <...Sparse...dtype 'int64'...shape (3, 1)>]\u001b[39;00m\n\u001b[32m    527\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    529\u001b[39m result = [_make_indexable(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m iterables]\n\u001b[32m--> \u001b[39m\u001b[32m530\u001b[39m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    531\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\notebooks\\titanic\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:473\u001b[39m, in \u001b[36mcheck_consistent_length\u001b[39m\u001b[34m(*arrays)\u001b[39m\n\u001b[32m    471\u001b[39m lengths = [_num_samples(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m arrays \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[32m    472\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(lengths)) > \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m473\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    474\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    475\u001b[39m         % [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[32m    476\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Found input variables with inconsistent numbers of samples: [55279, 70570]"
     ]
    }
   ],
   "source": [
    "# ===== Train XGB, RF, SVM (calibrated), Logistic; ensemble top-3; save if improved =====\n",
    "# Requirements: scikit-learn, xgboost, joblib, numpy\n",
    "# Run in same session where X_train_cls, X_val_cls, X_test_cls exist or .npy files exist.\n",
    "\n",
    "# If xgboost not installed, install it (uncomment if needed)\n",
    "# !pip install xgboost\n",
    "\n",
    "import os, joblib, warnings\n",
    "import numpy as np\n",
    "from time import time\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ---------------- Load embeddings and labels ----------------\n",
    "try:\n",
    "    X_train_cls  # noqa\n",
    "    X_val_cls\n",
    "    X_test_cls\n",
    "    print(\"Embeddings found in memory.\")\n",
    "except NameError:\n",
    "    print(\"Loading embeddings from disk...\")\n",
    "    X_train_cls = np.load(\"inlegalbert_cls_embeddings/X_train_cls.npy\")\n",
    "    X_val_cls = np.load(\"inlegalbert_cls_embeddings/X_val_cls.npy\")\n",
    "    X_test_cls = np.load(\"inlegalbert_cls_embeddings/X_test_cls.npy\")\n",
    "\n",
    "y_train = train[\"label\"].to_numpy()\n",
    "y_val = val[\"label\"].to_numpy()\n",
    "y_test = test[\"label\"].to_numpy()\n",
    "\n",
    "print(\"Shapes:\", X_train_cls.shape, X_val_cls.shape, X_test_cls.shape)\n",
    "print(\"Label counts (train):\", np.bincount(y_train))\n",
    "\n",
    "# Combine train+val for ensemble re-fitting later\n",
    "X_ens_train_raw = np.vstack([X_train_cls, X_val_cls])\n",
    "y_ens_train = np.concatenate([y_train, y_val])\n",
    "\n",
    "# Prepare scaled versions for methods that need it\n",
    "scaler_for_log = StandardScaler()\n",
    "X_train_log = scaler_for_log.fit_transform(X_train_cls)\n",
    "X_val_log = scaler_for_log.transform(X_val_cls)\n",
    "X_test_log = scaler_for_log.transform(X_test_cls)\n",
    "# Save scaler for logistic usage later\n",
    "os.makedirs(\"models_compare\", exist_ok=True)\n",
    "joblib.dump(scaler_for_log, \"models_compare/scaler_for_log.joblib\")\n",
    "\n",
    "# ---------------- Load previous saved model's test f1 (if exists) ----------------\n",
    "prev_model_path = \"inlegalbert_svm_model/linear_svc_best.joblib\"\n",
    "prev_best_f1 = None\n",
    "if os.path.exists(prev_model_path):\n",
    "    try:\n",
    "        prev_model = joblib.load(prev_model_path)\n",
    "        # if there's a scaler saved for SVM, use it; else use X_test_log as fallback\n",
    "        prev_scaler_path = \"inlegalbert_svm_model/scaler.joblib\"\n",
    "        if os.path.exists(prev_scaler_path):\n",
    "            prev_scaler = joblib.load(prev_scaler_path)\n",
    "            X_test_for_prev = prev_scaler.transform(X_test_cls)\n",
    "        else:\n",
    "            X_test_for_prev = X_test_log\n",
    "        y_prev_pred = prev_model.predict(X_test_for_prev)\n",
    "        prev_best_f1 = f1_score(y_test, y_prev_pred, average=\"macro\")\n",
    "        print(f\"Existing saved model test macro-F1: {prev_best_f1:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(\"Could not evaluate existing saved model:\", e)\n",
    "        prev_best_f1 = None\n",
    "\n",
    "# ---------------- Define models & parameter grids ----------------\n",
    "# Logistic Regression (pipeline with scaler)\n",
    "pipe_log = Pipeline([(\"scaler\", StandardScaler()), (\"clf\", LogisticRegression(solver=\"saga\", max_iter=2000, random_state=42))])\n",
    "param_grid_log = {\"clf__C\": [0.01, 0.1, 1.0, 10.0]}\n",
    "\n",
    "# Linear SVC (pipeline with scaler) - we will calibrate probabilities after selecting best C\n",
    "pipe_svc = Pipeline([(\"scaler\", StandardScaler()), (\"clf\", LinearSVC(max_iter=20000, tol=1e-4, dual=False))])  # dual=False recommended when n_samples > n_features\n",
    "param_grid_svc = {\"clf__C\": [0.01, 0.1, 1.0, 10.0]}\n",
    "\n",
    "# Random Forest (raw embeddings)\n",
    "rf = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "param_grid_rf = {\"n_estimators\": [200, 400], \"max_depth\": [None, 20], \"min_samples_split\": [2, 5]}\n",
    "\n",
    "# XGBoost (raw embeddings) - try to import\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    xgb_available = True\n",
    "    xgb = XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\", random_state=42)\n",
    "    param_dist_xgb = {\n",
    "        \"n_estimators\": [100, 200, 400],\n",
    "        \"max_depth\": [3, 6],\n",
    "        \"learning_rate\": [0.01, 0.05, 0.1],\n",
    "        \"subsample\": [0.7, 1.0],\n",
    "        \"colsample_bytree\": [0.7, 1.0],\n",
    "        \"reg_alpha\": [0, 0.1],\n",
    "        \"reg_lambda\": [1, 5]\n",
    "    }\n",
    "except Exception:\n",
    "    xgb_available = False\n",
    "    print(\"xgboost not installed — install with `pip install xgboost` to enable training XGBoost.\")\n",
    "\n",
    "# ---------------- Helper to fit and evaluate ----------------\n",
    "def run_grid_search(name, estimator, param_grid, Xtr, ytr, Xval, yval, Xte, yte, use_random=False, n_iter=20):\n",
    "    print(f\"\\n>> Running search for {name} ...\")\n",
    "    t0 = time()\n",
    "    if use_random:\n",
    "        search = RandomizedSearchCV(estimator, param_distributions=param_grid, n_iter=n_iter, scoring=\"f1_macro\", cv=3, n_jobs=-1, verbose=2, random_state=42)\n",
    "    else:\n",
    "        search = GridSearchCV(estimator, param_grid, scoring=\"f1_macro\", cv=3, n_jobs=-1, verbose=2)\n",
    "    search.fit(Xtr, ytr)\n",
    "    elapsed = time() - t0\n",
    "    best = search.best_estimator_\n",
    "    # Evaluate on val and test\n",
    "    yval_pred = best.predict(Xval)\n",
    "    ytest_pred = best.predict(Xte)\n",
    "    val_f1 = f1_score(yval, yval_pred, average=\"macro\")\n",
    "    test_f1 = f1_score(yte, ytest_pred, average=\"macro\")\n",
    "    print(f\"{name} done. Best params: {search.best_params_} | val_f1: {val_f1:.4f} | test_f1: {test_f1:.4f} | time: {elapsed:.1f}s\")\n",
    "    print(\"Val classification report:\")\n",
    "    print(classification_report(yval, yval_pred))\n",
    "    print(\"Test classification report:\")\n",
    "    print(classification_report(yte, ytest_pred))\n",
    "    return {\"name\": name, \"search\": search, \"best_estimator\": best, \"val_f1\": val_f1, \"test_f1\": test_f1, \"time_s\": elapsed}\n",
    "\n",
    "# ---------------- 1) Train models ----------------\n",
    "results = []\n",
    "\n",
    "# Logistic (on scaled embeddings)\n",
    "res_log = run_grid_search(\"LogisticRegression\", pipe_log, param_grid_log, X_train_cls, y_train, X_val_cls, y_val, X_test_cls, y_test, use_random=False)\n",
    "results.append(res_log)\n",
    "\n",
    "# Linear SVC (on scaled embeddings)\n",
    "res_svc = run_grid_search(\"LinearSVC\", pipe_svc, param_grid_svc, X_train_cls, y_train, X_val_cls, y_val, X_test_cls, y_test, use_random=False)\n",
    "# Calibrate SVC with validation set to enable predict_proba for ensemble\n",
    "print(\"\\nCalibrating LinearSVC for probabilities using validation set (Platt scaling)...\")\n",
    "best_svc_prefit = res_svc[\"best_estimator\"]\n",
    "calibrated_svc = CalibratedClassifierCV(best_svc_prefit.named_steps[\"clf\"], method=\"sigmoid\", cv=\"prefit\")\n",
    "# need to fit a pipeline wrapper that includes scaler: we will build a pipeline where scaler is same as in pipe_svc\n",
    "# fit scaler on train\n",
    "scaler_svc = best_svc_prefit.named_steps[\"scaler\"]\n",
    "# fit calibrated classifier on scaled val\n",
    "X_val_scaled_for_calib = scaler_svc.transform(X_val_cls)\n",
    "calibrated_svc.fit(scaler_svc.transform(X_train_cls), y_train)  # calibrate using train (CV='prefit' requires base estimator to be already fitted; here we fit on train to calibrate)\n",
    "# instead prefer calibrate with val for Platt scaling; to keep it consistent with scikit-learn API we calibrate using cv='prefit' by first fitting base then calibrating on val:\n",
    "# Refit base on train (already done by GridSearchCV) and calibrate on val:\n",
    "calibrated_svc = CalibratedClassifierCV(best_svc_prefit.named_steps[\"clf\"], method=\"sigmoid\", cv=\"prefit\")\n",
    "calibrated_svc.fit(scaler_svc.transform(X_val_cls), y_val)  # calibrate on val\n",
    "# Wrap calibrated classifier with same scaler into pipeline for predict_proba\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "class ScaledCalibratedClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, scaler, calibrated_clf):\n",
    "        self.scaler = scaler\n",
    "        self.calibrated_clf = calibrated_clf\n",
    "    def fit(self, X, y):\n",
    "        Xs = self.scaler.transform(X)\n",
    "        self.calibrated_clf.fit(Xs, y)\n",
    "        return self\n",
    "    def predict(self, X):\n",
    "        return self.calibrated_clf.predict(self.scaler.transform(X))\n",
    "    def predict_proba(self, X):\n",
    "        return self.calibrated_clf.predict_proba(self.scaler.transform(X))\n",
    "\n",
    "svc_pipeline_with_calib = ScaledCalibratedClassifier(scaler_svc, calibrated_svc)\n",
    "# Evaluate calibrated svc on test\n",
    "ytest_pred_svc = svc_pipeline_with_calib.predict(X_test_cls)\n",
    "svc_test_f1 = f1_score(y_test, ytest_pred_svc, average=\"macro\")\n",
    "print(f\"Calibrated LinearSVC TEST macro-F1: {svc_test_f1:.4f}\")\n",
    "res_svc[\"calibrated_pipeline\"] = svc_pipeline_with_calib\n",
    "res_svc[\"calibrated_test_f1\"] = svc_test_f1\n",
    "results.append(res_svc)\n",
    "\n",
    "# RandomForest (raw)\n",
    "res_rf = run_grid_search(\"RandomForest\", rf, param_grid_rf, X_train_cls, y_train, X_val_cls, y_val, X_test_cls, y_test, use_random=False)\n",
    "results.append(res_rf)\n",
    "\n",
    "# XGBoost (raw) - randomized search for speed\n",
    "if xgb_available:\n",
    "    res_xgb = run_grid_search(\"XGBoost\", xgb, param_dist_xgb, X_train_cls, y_train, X_val_cls, y_val, X_test_cls, y_test, use_random=True, n_iter=20)\n",
    "    results.append(res_xgb)\n",
    "else:\n",
    "    print(\"\\nSkipping XGBoost training (not available). Install xgboost to enable.\")\n",
    "\n",
    "# ---------------- 2) Consolidate test f1 results ----------------\n",
    "summary = []\n",
    "for r in results:\n",
    "    # for SVC prefer calibrated_test_f1\n",
    "    if r[\"name\"] == \"LinearSVC\":\n",
    "        test_f1 = r.get(\"calibrated_test_f1\", r[\"test_f1\"])\n",
    "    else:\n",
    "        test_f1 = r[\"test_f1\"]\n",
    "    summary.append((r[\"name\"], test_f1, r))\n",
    "summary_sorted = sorted(summary, key=lambda x: x[1], reverse=True)\n",
    "print(\"\\n=== Models sorted by TEST macro-F1 ===\")\n",
    "for name, f1v, _ in summary_sorted:\n",
    "    print(f\"{name}: test_f1 = {f1v:.4f}\")\n",
    "\n",
    "# ---------------- 3) Build ensemble from top-3 models ----------------\n",
    "top_k = 3\n",
    "top_models = summary_sorted[:top_k]\n",
    "ensemble_estimators = []\n",
    "print(\"\\nTop models to ensemble (in order):\", [(m[0], m[1]) for m in top_models])\n",
    "\n",
    "# For ensemble we will re-create estimator instances with best hyperparameters and fit them on train+val\n",
    "# Helper factory to build estimator object with best params and ready for fitting on raw embeddings\n",
    "def build_estimator_for_ensemble(info):\n",
    "    name = info[\"name\"]\n",
    "    search = info[\"search\"]\n",
    "    best_params = search.best_params_\n",
    "    if name == \"LogisticRegression\":\n",
    "        C = best_params.get(\"clf__C\", 1.0)\n",
    "        est = Pipeline([(\"scaler\", StandardScaler()), (\"clf\", LogisticRegression(C=C, solver=\"saga\", max_iter=2000, random_state=42))])\n",
    "    elif name == \"LinearSVC\":\n",
    "        C = best_params.get(\"clf__C\", 1.0)\n",
    "        base = Pipeline([(\"scaler\", StandardScaler()), (\"clf\", LinearSVC(C=C, max_iter=20000, dual=False))])\n",
    "        # We'll create a calibrated pipeline using CalibratedClassifierCV (not prefit) to allow predict_proba\n",
    "        # Wrap: first scaler + linearSVC, but CalibratedClassifierCV expects an unfitted base estimator, so we'll build fine\n",
    "        # Create a pipeline-like object: scaler then calibrated clf - we'll implement by fitting scaler separately below\n",
    "        est = (\"LinearSVC_calibrated\", C)  # placeholder, handled below\n",
    "    elif name == \"RandomForest\":\n",
    "        # pass through best params\n",
    "        bp = search.best_params_\n",
    "        est = RandomForestClassifier(n_estimators=bp.get(\"n_estimators\",200), max_depth=bp.get(\"max_depth\", None), min_samples_split=bp.get(\"min_samples_split\",2), random_state=42, n_jobs=-1)\n",
    "    elif name == \"XGBoost\":\n",
    "        bp = search.best_params_\n",
    "        est = XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\",\n",
    "                            n_estimators=bp.get(\"n_estimators\",100),\n",
    "                            max_depth=bp.get(\"max_depth\",3),\n",
    "                            learning_rate=bp.get(\"learning_rate\",0.1),\n",
    "                            subsample=bp.get(\"subsample\",1.0),\n",
    "                            colsample_bytree=bp.get(\"colsample_bytree\",1.0),\n",
    "                            reg_alpha=bp.get(\"reg_alpha\",0),\n",
    "                            reg_lambda=bp.get(\"reg_lambda\",1),\n",
    "                            random_state=42)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown model for ensemble: \" + name)\n",
    "    return est\n",
    "\n",
    "# Build actual estimators and fit them on train+val\n",
    "fitted_estimators = []\n",
    "for name, f1v, r in top_models:\n",
    "    print(f\"\\nFitting ensemble member: {name} on train+val\")\n",
    "    if name == \"LinearSVC\":\n",
    "        # create scaler + base SVC, then calibrate (CalibratedClassifierCV) to get predict_proba\n",
    "        C = r[\"search\"].best_params_.get(\"clf__C\", 1.0)\n",
    "        scaler = StandardScaler().fit(X_ens_train_raw)  # fit scaler on combined data\n",
    "        X_ens_train_scaled = scaler.transform(X_ens_train_raw)\n",
    "        base = LinearSVC(C=C, max_iter=20000, dual=False)\n",
    "        base.fit(X_ens_train_scaled, y_ens_train)\n",
    "        calib = CalibratedClassifierCV(base, method=\"sigmoid\", cv=\"prefit\")\n",
    "        # calibrate using a holdout from train+val: we'll calibrate using the same combined data (ok), or better use val only.\n",
    "        # To keep things simple we calibrate on the combined set:\n",
    "        calib.fit(X_ens_train_scaled, y_ens_train)\n",
    "        # wrap into ScaledCalibratedClassifier for consistent API\n",
    "        est = ScaledCalibratedClassifier(scaler, calib)\n",
    "        fitted_estimators.append((\"LinearSVC_calibrated\", est))\n",
    "    elif name == \"LogisticRegression\":\n",
    "        C = r[\"search\"].best_params_.get(\"clf__C\", 1.0)\n",
    "        pipeline = Pipeline([(\"scaler\", StandardScaler()), (\"clf\", LogisticRegression(C=C, solver=\"saga\", max_iter=2000, random_state=42))])\n",
    "        pipeline.fit(X_ens_train_raw, y_ens_train)\n",
    "        fitted_estimators.append((\"Logistic\", pipeline))\n",
    "    elif name == \"RandomForest\":\n",
    "        bp = r[\"search\"].best_params_\n",
    "        rf_est = RandomForestClassifier(n_estimators=bp.get(\"n_estimators\",200), max_depth=bp.get(\"max_depth\", None), min_samples_split=bp.get(\"min_samples_split\",2), random_state=42, n_jobs=-1)\n",
    "        rf_est.fit(X_ens_train_raw, y_ens_train)\n",
    "        fitted_estimators.append((\"RandomForest\", rf_est))\n",
    "    elif name == \"XGBoost\" and xgb_available:\n",
    "        bp = r[\"search\"].best_params_\n",
    "        xgb_est = XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\",\n",
    "                                n_estimators=bp.get(\"n_estimators\",100),\n",
    "                                max_depth=bp.get(\"max_depth\",3),\n",
    "                                learning_rate=bp.get(\"learning_rate\",0.1),\n",
    "                                subsample=bp.get(\"subsample\",1.0),\n",
    "                                colsample_bytree=bp.get(\"colsample_bytree\",1.0),\n",
    "                                reg_alpha=bp.get(\"reg_alpha\",0),\n",
    "                                reg_lambda=bp.get(\"reg_lambda\",1),\n",
    "                                random_state=42)\n",
    "        xgb_est.fit(X_ens_train_raw, y_ens_train)\n",
    "        fitted_estimators.append((\"XGBoost\", xgb_est))\n",
    "    else:\n",
    "        print(f\"Skipping ensemble build for {name} (unknown or unavailable).\")\n",
    "\n",
    "print(\"\\nFitted estimators for ensemble:\", [n for n,_ in fitted_estimators])\n",
    "\n",
    "# Now create VotingClassifier with soft voting (requires predict_proba for all)\n",
    "vc = VotingClassifier(estimators=fitted_estimators, voting=\"soft\", n_jobs=-1)\n",
    "# VotingClassifier.fit will attempt to re-fit estimators — they are already fitted; to avoid double-fitting,\n",
    "# we'll instead implement predict_proba by averaging member predict_proba manually.\n",
    "def ensemble_predict_proba(X):\n",
    "    probs = None\n",
    "    for _, est in fitted_estimators:\n",
    "        p = est.predict_proba(X)\n",
    "        if probs is None:\n",
    "            probs = p\n",
    "        else:\n",
    "            probs += p\n",
    "    probs /= len(fitted_estimators)\n",
    "    return probs\n",
    "\n",
    "# Evaluate ensemble on test\n",
    "probs_test = ensemble_predict_proba(X_test_cls)\n",
    "ytest_pred_ensemble = np.argmax(probs_test, axis=1)\n",
    "ensemble_test_f1 = f1_score(y_test, ytest_pred_ensemble, average=\"macro\")\n",
    "print(f\"\\nEnsemble TEST macro-F1: {ensemble_test_f1:.4f}\")\n",
    "print(\"Ensemble Test classification report:\")\n",
    "print(classification_report(y_test, ytest_pred_ensemble))\n",
    "\n",
    "# ---------------- Compare ensemble and best single model to previous saved model ----------------\n",
    "# Find best single model among trained ones\n",
    "best_single = max(summary_sorted, key=lambda x: x[1])  # tuple (name, f1, r)\n",
    "best_single_name, best_single_f1, best_single_info = best_single\n",
    "print(f\"\\nBest single trained model: {best_single_name} with TEST macro-F1 = {best_single_f1:.4f}\")\n",
    "print(f\"Ensemble TEST macro-F1 = {ensemble_test_f1:.4f}\")\n",
    "if prev_best_f1 is not None:\n",
    "    print(f\"Previous saved model TEST macro-F1 = {prev_best_f1:.4f}\")\n",
    "\n",
    "# Decide what to save:\n",
    "# - If ensemble_test_f1 > max(prev_best_f1, best_single_f1) then save ensemble\n",
    "# - Else if best_single_f1 > prev_best_f1 then save best single model\n",
    "# - Else keep previous\n",
    "to_overwrite = False\n",
    "if prev_best_f1 is None:\n",
    "    baseline = -1.0\n",
    "else:\n",
    "    baseline = prev_best_f1\n",
    "\n",
    "if ensemble_test_f1 > max(baseline, best_single_f1):\n",
    "    # Save ensemble\n",
    "    save_path = \"models_compare/best_model_ensemble_soft.joblib\"\n",
    "    # Save fitted_estimators and a small wrapper that uses them\n",
    "    wrapper = {\"estimators\": fitted_estimators}\n",
    "    joblib.dump(wrapper, save_path)\n",
    "    print(f\"Ensemble outperforms previous and best single. Saved ensemble wrapper to: {save_path}\")\n",
    "    # Overwrite previous model path as well\n",
    "    joblib.dump(wrapper, prev_model_path)\n",
    "    print(f\"Overwrote previous saved model at: {prev_model_path} with ensemble wrapper.\")\n",
    "elif best_single_f1 > baseline:\n",
    "    # Save best single model object (fitted estimator) to models_compare and overwrite previous model\n",
    "    name, f1v, info = best_single\n",
    "    if name == \"LinearSVC\":\n",
    "        # save calibrated pipeline\n",
    "        joblib.dump(res_svc[\"calibrated_pipeline\"], \"models_compare/best_model_LinearSVC_calibrated.joblib\")\n",
    "        joblib.dump(res_svc[\"calibrated_pipeline\"], prev_model_path)\n",
    "        print(f\"Saved calibrated LinearSVC and overwrote previous model at {prev_model_path}\")\n",
    "    else:\n",
    "        joblib.dump(info[\"best_estimator\"], f\"models_compare/best_model_{name}.joblib\")\n",
    "        joblib.dump(info[\"best_estimator\"], prev_model_path)\n",
    "        print(f\"Saved best single model ({name}) and overwrote previous model at {prev_model_path}\")\n",
    "else:\n",
    "    print(\"No trained model or ensemble improved upon the previously saved model. No overwrite performed.\")\n",
    "\n",
    "print(\"\\nDone. Artifacts saved in models_compare/ and inlegalbert_svm_model/ (if overwritten).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4dd0dd-33dd-4c3e-9ecc-ea8dc81de174",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
